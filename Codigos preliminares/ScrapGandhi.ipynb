{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mHU3AVyyvvc9",
        "38tHSUy6kzF4",
        "iW-46B4gktbt"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Importar librerias"
      ],
      "metadata": {
        "id": "0n_rFkPYkEdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqAVXi9Iqo0v",
        "outputId": "cff55609-b430-4463-c1dd-1ee0341cc965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=c0f4017ce659861c67263b4e976c1171e6f64f779a7907569eafa757380e9234\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import pickle\n",
        "import os\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from langdetect import detect, DetectorFactory"
      ],
      "metadata": {
        "id": "cei4khVVE9pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download es_core_news_sm"
      ],
      "metadata": {
        "id": "YYqOr7NfisfH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "012d8046-e64d-4e38-a0e7-f3dd29b78d1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-20 17:36:47.726616: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-20 17:36:47.726677: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-20 17:36:47.729462: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-20 17:36:49.371939: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting es-core-news-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.6.0/es_core_news_sm-3.6.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.1.3)\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-F4hko6tM-Lv",
        "outputId": "eab10e53-8ab8-4bfb-9dd4-0d0235b90bfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ajustables"
      ],
      "metadata": {
        "id": "852rf90VkMwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encabezado de las solicitudes http\n",
        "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
        "\n",
        "# Generos y paginas correspondientes a la página de Gandhi\n",
        "generos_num_paginas =[(\"ensayo-literario\", 11),\n",
        "          (\"ciencia-ficcion\", 4),\n",
        "          (\"terror-y-suspenso\", 8),\n",
        "          (\"arte-ii\", 14),\n",
        "          (\"arquitectura\", 3),\n",
        "          (\"cine\", 6),\n",
        "          (\"fotografia\", 6),\n",
        "          (\"desarrollo-humano\", 8),\n",
        "          (\"salud-y-ejercicio\", 5),\n",
        "          (\"negocios-y-finanzas\", 4),\n",
        "          (\"derecho\", 14),\n",
        "          (\"psicologia-psicoanalisis-psiquiatra\", 2),\n",
        "          (\"filosofia\", 17),\n",
        "          (\"viajes\", 3),\n",
        "          (\"gastronomia\", 3)]\n",
        "\n",
        "ruta_libros = '/content/drive/MyDrive/ESCOM/8vo Semestre/NLP/libros.csv'\n",
        "ruta_enlaces_invalidos = '/content/drive/MyDrive/ESCOM/8vo Semestre/NLP/enlaceinvalido.csv'\n",
        "ruta_libros_limpiado = '/content/drive/MyDrive/ESCOM/8vo Semestre/NLP/librosLimpiado.csv'\n",
        "ruta_libros_limpiado_esp = '/content/drive/MyDrive/ESCOM/8vo Semestre/NLP/librosLimpiadoEN.csv'"
      ],
      "metadata": {
        "id": "CoOAcu7_v8l9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Obtención de enlaces de libros"
      ],
      "metadata": {
        "id": "Z4MZa6zYkah8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urls_grid = []\n",
        "enlaces_libros = set()\n",
        "\n",
        "# Genera los enlaces de las paginas de cada genero para hacer el web scrapping\n",
        "urls_grid = [(f'https://www.gandhi.com.mx/libros/{genero}?p={num_pagina}', genero) for genero, num_paginas in generos_num_paginas for num_pagina in range(1, num_paginas + 1)]\n",
        "\n",
        "\n",
        "for url_g, genero in urls_grid:\n",
        "    response = requests.get(url_g, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Buscar todos los elementos 'a' y extraer el atributo 'href'\n",
        "    for enlace in soup.find_all('a', href=True):\n",
        "        aux = (enlace['href'], genero)\n",
        "        enlaces_libros.add(aux)\n",
        "        print(aux)\n",
        "\n",
        "    time.sleep(1)\n",
        "\n",
        "# La expresión regular que coincide con URLs de Gandhi sin ruta adicional\n",
        "# pattern = r\"https://www.gandhi.com.mx/[^/]+$\"\n",
        "pattern = r\"https://www.gandhi.com.mx/.+\"\n",
        "\n",
        "# Filtrar las URLs que coinciden con la expresión regular\n",
        "enlaces_libros = [(url, genero) for url, genero in enlaces_libros if re.match(pattern, url)]\n",
        "\n",
        "# for enlace in enlaces_libros:\n",
        "#   print(enlace)"
      ],
      "metadata": {
        "id": "IEpH2Lb9PKUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(enlaces_libros)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8I_B7O5i-uW4",
        "outputId": "b7ed7b32-ff7f-4502-a73a-933923139e7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9528"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Guardar los enlaces de los libros en un .pkl"
      ],
      "metadata": {
        "id": "lQlfNaw3voU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('enlaces_libros.pkl', 'wb') as file:\n",
        "    pickle.dump(enlaces_libros, file)"
      ],
      "metadata": {
        "id": "HFYop7AZveok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cargar los enlaces de los libros desde un .pkl"
      ],
      "metadata": {
        "id": "mHU3AVyyvvc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('enlaces_libros.pkl', 'rb') as file:\n",
        "    enlaces_libros = pickle.load(file)"
      ],
      "metadata": {
        "id": "Sb8sf8aDvm6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scrapping individual"
      ],
      "metadata": {
        "id": "38tHSUy6kzF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enlace_libro = \"https://www.gandhi.com.mx/pasiones-toxicas\"\n",
        "response = requests.get(enlace_libro, headers=headers)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Extraer el ISBN\n",
        "ISBN_tag = soup.find('div', class_='isbn').find_all('span')[1]\n",
        "ISBN = ISBN_tag.get_text(strip=True) if ISBN_tag else ''\n",
        "\n",
        "# Extraer el título\n",
        "titulo_tag = soup.find('h1', class_='page-title')\n",
        "titulo = titulo_tag.get_text(strip=True) if titulo_tag else ''\n",
        "\n",
        "# Extraer el nombre del autor\n",
        "autor_tag = soup.find('div', class_='autor').find('a')\n",
        "autor = autor_tag.get_text(strip=True) if autor_tag else ''\n",
        "\n",
        "# Extraer la editorial\n",
        "editorial_tag = soup.find('div', class_='editoriales').find('a')\n",
        "editorial = editorial_tag.get_text(strip=True) if editorial_tag else ''\n",
        "\n",
        "# Extraer el número de páginas\n",
        "num_paginas_tag = soup.find('li', {'data-li': 'Número de páginas'})\n",
        "num_paginas = num_paginas_tag.find('span', class_='attr-data').text if num_paginas_tag else ''\n",
        "\n",
        "# Extraer la fecha de publicación\n",
        "fecha_publicacion_tag = soup.find('li', {'data-li': 'Fecha de publicación'})\n",
        "fecha_publicacion = fecha_publicacion_tag.find('span', class_='attr-data').text if fecha_publicacion_tag else ''\n",
        "\n",
        "# Extraer la sinopsis\n",
        "sinopsis_tag = soup.find('div', id='description')\n",
        "sinopsis = sinopsis_tag.get_text(strip=True) if sinopsis_tag else ''\n",
        "\n",
        "img_tag = soup.find('img', class_='product-photo')\n",
        "image_url = img_tag.get('data-srclazy') or img_tag.get('data-lazy') if img_tag else ''\n",
        "\n",
        "\n",
        "print([ISBN, titulo, autor, editorial, num_paginas, fecha_publicacion, sinopsis, image_url, enlace_libro])\n",
        "\n",
        "if all([ISBN, titulo, autor, editorial, num_paginas, fecha_publicacion, sinopsis, image_url, enlace_libro]):\n",
        "\n",
        "            print([ISBN, titulo, autor, editorial, num_paginas, fecha_publicacion, sinopsis, image_url, enlace_libro])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZjYSKi3xEfk",
        "outputId": "3bee7891-8ea0-4c53-f5ca-1e8b3fe4a208"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['9786075692180', 'Pasiones tóxicas', 'Bernardo Stamateas', 'Paidós', '272', ' 20 de Mayo 2022 ', 'Pasiones tóxicas Bernardo Stamateas Todas las parejas pasan por crisis, el tema es tener los recursos necesarios para atravesarlas. Porque cuando permanecemos atrapados y detenidos en este estado, los conflictos y la confusión llegarán a nosotros de manera inevitable. Por eso, en este libro analizaremos las pasiones más frecuentes de las parejas: Las pasiones del engaño y la infidelidad. Las pasiones de la posesividad. Las pasiones del estancamiento. Las pasiones de la competitividad. Las pasiones de la descalificación. La idea de Pasiones tóxicas es dar herramientas para repensar y activar los recursos que ya están adentro nuestro. Y es así como el licenciado Bernardo Stamateas echa abajo varios mitos que responden a idealizaciones románticas de la pareja, restituyendo ideas y conceptos más humanos: el conflicto y el enojo son normales, en una pareja nadie tiene la razón, estar casado es normal y estar soltero también es normal, la pareja es una lucha diaria, y muchos otros que el lector irá encontrando a lo largo de este libro vital para todo aquel que decida apostar al desafío que significa encarar la vida de a dos en un mundo cada vez más complejo. La pareja es una institución en crisis. Hace 35 años que vivo en una y 30 que me dedico a tratar de ayudarlas. De estas experiencias sé que la construcción de una relación así es un gran desafío, y como dice el licenciado Stamateas en su obra: Fortalecer el amor en estos tiempos se ha vuelto desafiante. Estoy seguro de que más de un lector se verá reflejado en las ideas de este libro y de esta forma, tal vez, se pueda sentir ayudado en ese desafío. Del prólogo del profesor Dr. Omar Biscotti, director del Instituto Sistémico de Buenos Aires', 'https://www.gandhi.com.mx/media/catalog/product/9/7/9786075692180_c1e9.jpg?optimize=high&bg-color=255,255,255&fit=bounds&height=478&width=300&canvas=300:478', 'https://www.gandhi.com.mx/pasiones-toxicas']\n",
            "['9786075692180', 'Pasiones tóxicas', 'Bernardo Stamateas', 'Paidós', '272', ' 20 de Mayo 2022 ', 'Pasiones tóxicas Bernardo Stamateas Todas las parejas pasan por crisis, el tema es tener los recursos necesarios para atravesarlas. Porque cuando permanecemos atrapados y detenidos en este estado, los conflictos y la confusión llegarán a nosotros de manera inevitable. Por eso, en este libro analizaremos las pasiones más frecuentes de las parejas: Las pasiones del engaño y la infidelidad. Las pasiones de la posesividad. Las pasiones del estancamiento. Las pasiones de la competitividad. Las pasiones de la descalificación. La idea de Pasiones tóxicas es dar herramientas para repensar y activar los recursos que ya están adentro nuestro. Y es así como el licenciado Bernardo Stamateas echa abajo varios mitos que responden a idealizaciones románticas de la pareja, restituyendo ideas y conceptos más humanos: el conflicto y el enojo son normales, en una pareja nadie tiene la razón, estar casado es normal y estar soltero también es normal, la pareja es una lucha diaria, y muchos otros que el lector irá encontrando a lo largo de este libro vital para todo aquel que decida apostar al desafío que significa encarar la vida de a dos en un mundo cada vez más complejo. La pareja es una institución en crisis. Hace 35 años que vivo en una y 30 que me dedico a tratar de ayudarlas. De estas experiencias sé que la construcción de una relación así es un gran desafío, y como dice el licenciado Stamateas en su obra: Fortalecer el amor en estos tiempos se ha vuelto desafiante. Estoy seguro de que más de un lector se verá reflejado en las ideas de este libro y de esta forma, tal vez, se pueda sentir ayudado en ese desafío. Del prólogo del profesor Dr. Omar Biscotti, director del Instituto Sistémico de Buenos Aires', 'https://www.gandhi.com.mx/media/catalog/product/9/7/9786075692180_c1e9.jpg?optimize=high&bg-color=255,255,255&fit=bounds&height=478&width=300&canvas=300:478', 'https://www.gandhi.com.mx/pasiones-toxicas']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scrapping por lotes"
      ],
      "metadata": {
        "id": "W5PqKeEtk6Ue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtecion de enlaces faltantes"
      ],
      "metadata": {
        "id": "VNKvgmx6Ph4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "libros_existentes = set()\n",
        "try:\n",
        "    with open(ruta_libros, mode='r', newline='', encoding='utf-8') as file:\n",
        "        reader = csv.reader(file)\n",
        "        next(reader)  # Saltar la cabecera\n",
        "        for fila in reader:\n",
        "            tupla = (fila[9], fila[2])\n",
        "            libros_existentes.add(tupla)\n",
        "except FileNotFoundError:\n",
        "    print(\"Archivo no encontrado, se creará uno nuevo.\")"
      ],
      "metadata": {
        "id": "-eMT9EeCqt-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enlaces_invalidos = set()\n",
        "try:\n",
        "    with open(ruta_enlaces_invalidos, mode='r', newline='', encoding='utf-8') as file:\n",
        "        reader = csv.reader(file)\n",
        "        next(reader)  # Saltar la cabecera\n",
        "        for fila in reader:\n",
        "            tupla = (fila[0], fila[1])\n",
        "            enlaces_invalidos.add(tupla)\n",
        "except FileNotFoundError:\n",
        "    print(\"Archivo no encontrado, se creará uno nuevo.\")"
      ],
      "metadata": {
        "id": "mZZEL0qhEcgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"enlaces_libros: \",len(enlaces_libros))\n",
        "print(\"enlaces_invalidos: \",len(enlaces_invalidos))\n",
        "print(\"libros_existentes: \",len(libros_existentes))\n",
        "enlaces_libros = set(enlaces_libros)\n",
        "enlaces_libros = enlaces_libros.difference(libros_existentes)\n",
        "enlaces_libros = enlaces_libros.difference(enlaces_invalidos)\n",
        "print(len(enlaces_libros))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVSGj6poMLTn",
        "outputId": "e96d9df5-bbf9-49e0-9ceb-fe62261a9bbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enlaces_libros:  993\n",
            "enlaces_invalidos:  6827\n",
            "libros_existentes:  2701\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracción de datos"
      ],
      "metadata": {
        "id": "Xhuf3gMiPpz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extraer_texto(soup, selector, attribute=None, all_spans=False, index=0):\n",
        "    tag = soup.select_one(selector)\n",
        "    if tag and all_spans:\n",
        "        spans = tag.find_all('span')\n",
        "        if spans and len(spans) > index:\n",
        "            return spans[index].get_text(strip=True)\n",
        "    return tag.get_text(strip=True) if tag else ''"
      ],
      "metadata": {
        "id": "KLJitKUYPzT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_exist = os.path.exists(ruta_libros)\n",
        "enlaces_error = []\n",
        "\n",
        "with open(ruta_libros, mode='a', newline='', encoding='utf-8') as file:\n",
        "    with open(ruta_enlaces_invalidos, mode='a', newline='', encoding='utf-8') as enlaces_file:\n",
        "      writer = csv.writer(file)\n",
        "      writer_enalces = csv.writer(enlaces_file)\n",
        "\n",
        "      if not file_exist:\n",
        "          writer.writerow(['ISBN', 'Título', 'Género', 'Autor', 'Editorial', 'Número de Páginas', 'Fecha de Publicación', 'Sinopsis', \"Portada\", \"URL\"])\n",
        "\n",
        "      for enlace_libro, genero in enlaces_libros:\n",
        "          response = requests.get(enlace_libro, headers=headers)\n",
        "          soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "          try:\n",
        "            ISBN = extraer_texto(soup, 'div.isbn', all_spans=True, index=1)\n",
        "            titulo = extraer_texto(soup, 'h1.page-title')\n",
        "            autor = extraer_texto(soup, 'div.autor a')\n",
        "            editorial = extraer_texto(soup, 'div.editoriales a')\n",
        "            num_paginas = extraer_texto(soup, 'li[data-li=\"Número de páginas\"] .attr-data')\n",
        "            fecha_publicacion = extraer_texto(soup, 'li[data-li=\"Fecha de publicación\"] .attr-data')\n",
        "            sinopsis = extraer_texto(soup, 'div#description')\n",
        "\n",
        "            img_tag = soup.find('img', class_='product-photo')\n",
        "            image_url = img_tag.get('data-srclazy') or img_tag.get('data-lazy') if img_tag else ''\n",
        "\n",
        "            if all([ISBN, titulo, genero, autor, editorial, num_paginas, fecha_publicacion, sinopsis, image_url, enlace_libro]):\n",
        "                writer.writerow([ISBN, titulo, genero, autor, editorial, num_paginas, fecha_publicacion, sinopsis, image_url, enlace_libro])\n",
        "                print([ISBN, titulo, genero, autor, editorial, num_paginas, fecha_publicacion, sinopsis, image_url, enlace_libro])\n",
        "            else:\n",
        "                writer_enalces.writerow([enlace_libro, genero])\n",
        "                print(\"Enlace invalido: \", enlace_libro)\n",
        "          except Exception as e:\n",
        "            print(\"Error al procesar la página:\", enlace_libro, \"\\nError:\", e)\n",
        "          # time.sleep(0.05)\n",
        "\n",
        "print(\"Datos guardados en libros.csv\")\n"
      ],
      "metadata": {
        "id": "jFwkoytiIWkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Limpieza de los datos"
      ],
      "metadata": {
        "id": "1gXwNK1-dO9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carga el archivo CSV\n",
        "df = pd.read_csv(ruta_libros)\n",
        "\n",
        "# Reemplaza 'ciencia-ficcion' por 'Ciencia ficción' en la columna deseada\n",
        "df['Género'] = df['Género'].str.replace('arquitectura', 'Arquitectura')\n",
        "df['Género'] = df['Género'].str.replace('arte-ii', 'Arte')\n",
        "df['Género'] = df['Género'].str.replace('ciencia-ficcion', 'Ciencia Ficción')\n",
        "df['Género'] = df['Género'].str.replace('cine', 'Cine')\n",
        "df['Género'] = df['Género'].str.replace('derecho', 'Derecho')\n",
        "df['Género'] = df['Género'].str.replace('desarrollo-humano', 'Desarrollo Humano')\n",
        "df['Género'] = df['Género'].str.replace('ensayo-literario', 'Ensayo Literario')\n",
        "df['Género'] = df['Género'].str.replace('filosofia', 'Filosofía')\n",
        "df['Género'] = df['Género'].str.replace('fotografia', 'Fotografía')\n",
        "df['Género'] = df['Género'].str.replace('gastronomia', 'Gastronomía')\n",
        "df['Género'] = df['Género'].str.replace('negocios-y-finanzas', 'Negocios y Finanzas')\n",
        "df['Género'] = df['Género'].str.replace('psicologia-psicoanalisis-psiquiatra', 'Psicología Psicoanalisis Psiquiatra')\n",
        "df['Género'] = df['Género'].str.replace('salud-y-ejercicio', 'Salud y Ejercicio')\n",
        "df['Género'] = df['Género'].str.replace('terror-y-suspenso', 'Terror y Suspenso')\n",
        "df['Género'] = df['Género'].str.replace('viajes', 'Viajes')"
      ],
      "metadata": {
        "id": "p9hpecmVets0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"es_core_news_sm\")"
      ],
      "metadata": {
        "id": "9QSXGLLYiX2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalizacion (cadena):\n",
        "  doc = nlp(cadena)\n",
        "  texto_normalizado = \"\"\n",
        "  for token in doc:\n",
        "    if(not token.is_stop and not token.pos_=='SPACE'):\n",
        "      texto_normalizado += f\" {token.lemma_}\"\n",
        "      print(token.text, token.pos_, token.dep_, token.lemma_)\n",
        "\n",
        "  return texto_normalizado[1:]"
      ],
      "metadata": {
        "id": "eVg7QIv2joix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Normalización'] = df.apply(lambda fila: normalizacion(f\"{fila['Título']} {fila['Género']} {fila['Autor']} {fila['Editorial']} {fila['Fecha de Publicación']} {fila['Sinopsis']}\"), axis=1)"
      ],
      "metadata": {
        "id": "CgcIdjgOhfpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(ruta_libros_limpiado)\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "df['Idioma'] = df['Normalización'].apply(detect)\n",
        "df = df[df['Idioma'] == 'es']\n",
        "df = df.drop(['Idioma'], axis=1)"
      ],
      "metadata": {
        "id": "tp-Irl1RpuaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guarda el archivo CSV modificado\n",
        "df.to_csv(ruta_libros_limpiado, index=False)"
      ],
      "metadata": {
        "id": "gGJfV-GKlI8L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}